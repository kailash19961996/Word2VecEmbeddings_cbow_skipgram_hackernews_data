{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet data\n",
    "data = pd.read_parquet('hackernews.parquet')\n",
    "# Now, you can use the `data' DataFrame to analyse and manipulate the data.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns with the correct rows\n",
    "story_data = data[data[\"type\"] == \"story\"]\n",
    "\n",
    "# Extract the relevant columns\n",
    "relevant_data = story_data[[\"title\", \"score\"]]\n",
    "\n",
    "# Remove any null entries\n",
    "processed_data = relevant_data.dropna()\n",
    "\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "data_array = processed_data.to_numpy()\n",
    "print(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data with sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the titles together one by one\n",
    "corpus = \"\\n\".join(data_array[:, 0])\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles\n",
    "title_array = data_array[:, 0]\n",
    "\n",
    "# Convert corpus to a file\n",
    "corpus = \"\\n\".join(title_array)\n",
    "with open(\"corpus.txt\", \"w\") as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=\"corpus.txt\", model_prefix='hackernews', vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('hackernews.model')\n",
    "\n",
    "# Tokenize titles\n",
    "tokenized_titles = [sp.encode_as_pieces(title) for title in title_array]\n",
    "\n",
    "# Turn the tokens into tokenids\n",
    "id_titles = [[sp.piece_to_id(token) for token in title_tokens] for title_tokens in tokenized_titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply word2vec to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = sp.get_piece_size()\n",
    "embedding_dim = 128\n",
    "window_size = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the pairs of tokens and context windows for CBOW\n",
    "def generate_cbow_pairs(tokenized_sentences, window_size=2):\n",
    "    cbow_pairs = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context = [sentence[i] for i in range(start, end) if 0 <= i < sentence_length and i != index]\n",
    "            target = word\n",
    "            cbow_pairs.append((context, target))\n",
    "    return cbow_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `Dataset` class for generating pairs of tokens and context windows for CBOW one at a time, to use in the `DataLoader` to generate mini-batches\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_pairs, context_size):\n",
    "        self.cbow_pairs = cbow_pairs\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.cbow_pairs[idx]\n",
    "        # Pad or trim the context to ensure uniform size\n",
    "        if len(context) < 2 * self.context_size:\n",
    "            context += [0] * (2 * self.context_size - len(context))  # Assuming 0 is the padding index\n",
    "        else:\n",
    "            context = context[:2 * self.context_size]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CBOW model, which is just an embedding layer, an average pooling, and a final output linear layer followed by a softmax activation\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        embedded = self.embeddings(context_words)\n",
    "        projection = torch.mean(embedded, dim=1)\n",
    "        out = self.linear(projection)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def embed(self, word):\n",
    "        self.eval()\n",
    "        embedding = self.embeddings(word)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the data generating functions we wrote earlier to our hacker news data\n",
    "cbow_pairs = generate_cbow_pairs(id_titles, window_size)\n",
    "cbow_dataset = CBOWDataset(cbow_pairs, window_size)\n",
    "cbow_dataloader = DataLoader(cbow_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "cbow_model = CBOWModel(vocab_size, embedding_dim)  # Assuming CBOWModel is defined as before\n",
    "cbow_loss_function = nn.NLLLoss()\n",
    "cbow_optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    with tqdm(cbow_dataloader, unit=\"batch\") as tepoch:\n",
    "        for context, target in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            cbow_model.zero_grad()\n",
    "            log_probs = cbow_model(context)\n",
    "            loss = cbow_loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            cbow_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished with total loss: {total_loss:.4f}\")\n",
    "    torch.save(cbow_model.state_dict(), os.path.join(\"cbow_weights\", f\"cbow_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the pairs of tokens and context windows for Skip-gram\n",
    "def generate_skip_gram_pairs(tokenized_sentences, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                center_word_idx = sentence[center_word_pos]\n",
    "                context_word_idx = sentence[context_word_pos]\n",
    "                pairs.append((center_word_idx, context_word_idx))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `Dataset` class for generating pairs of tokens and context windows for Skip-gram one at a time, to use in the `DataLoader` to generate mini-batches\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the data generating functions we wrote earlier to our hacker news data\n",
    "sg_pairs = generate_skip_gram_pairs(id_titles, window_size)\n",
    "sg_dataset = SkipGramDataset(sg_pairs)\n",
    "sg_dataloader = DataLoader(sg_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Skip-gram model, which is just an embedding layer, and a final output linear layer followed by a softmax activation\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word):\n",
    "        embedded = self.embeddings(target_word)\n",
    "        out = self.linear(embedded)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def embed(self, word):\n",
    "        self.eval()\n",
    "        embedding = self.embeddings(word)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "sg_model = SkipGramModel(vocab_size, embedding_dim)\n",
    "sg_loss_function = nn.NLLLoss()\n",
    "sg_optimizer = optim.Adam(sg_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center_word, context_word in tqdm(sg_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        sg_optimizer.zero_grad()\n",
    "        log_probs = sg_model(center_word)\n",
    "        loss = sg_loss_function(log_probs, context_word)\n",
    "        loss.backward()\n",
    "        sg_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Total Loss = {total_loss}\")\n",
    "\n",
    "    # Save the model state after each epoch\n",
    "    torch.save(sg_model.state_dict(), os.path.join(\"skipgram_weights\", f\"skipgram_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `Dataset` class for generating pairs of id-tokenised titles and scores one at a time, to use in the `DataLoader` to generate mini-batches\n",
    "class TitlesAndScores(Dataset):\n",
    "    def __init__(self, titles, scores):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids (list of list of words): Nested list where each sublist contains token IDs for a sentence.\n",
    "            scores (list of float): List of scores associated with each list of token IDs.\n",
    "        \"\"\"\n",
    "        self.titles = titles\n",
    "        self.tokenized_titles = [sp.encode_as_pieces(title) for title in self.titles]\n",
    "        self.id_titles = [[sp.piece_to_id(token) for token in title_tokens] for title_tokens in self.tokenized_titles]\n",
    "        self.scores = scores\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.id_titles[idx], self.scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A padding function to make sure that every list of title token ids in a mini-batch is the same length, so that the batch can be tensorised\n",
    "def pad_collate(batch):\n",
    "    (id_titles, scores) = zip(*batch)\n",
    "    \n",
    "    # Padding the sequences with 0\n",
    "    padded_titles = pad_sequence([torch.tensor(title_ids) for title_ids in id_titles], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert scores to a tensor\n",
    "    tensor_scores = torch.tensor(scores, dtype=torch.float)\n",
    "\n",
    "    return padded_titles, tensor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset on our hackernews data\n",
    "sp_dataset = TitlesAndScores(data_array[:,0], data_array[:,1])\n",
    "\n",
    "# Create the dataLoader on our hackernews data\n",
    "sp_dataloader = DataLoader(sp_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model for predicting scores given titles. \n",
    "# This particular architecture is intended to use one of the word2vec models we trained earlier (CBOW or Skip-gram) -- with its weights frozen -- to embed the input title tokens.\n",
    "# It then uses a classic neural network architecture to predict the scores given the embeddings.\n",
    "# This architecture has two hidden layers with ReLU activations, and a final output layer with no activation (a linear output layer) because we are doing a regression task. \n",
    "class ScorePredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, model, weights):\n",
    "        super(ScorePredictor, self).__init__()\n",
    "        self.embedding_model = model(vocab_size, embedding_dim)\n",
    "        self.embedding_model.load_state_dict(torch.load(weights))\n",
    "        self.embedding_model.eval()\n",
    "\n",
    "        self.hidden_1 = nn.Linear(embedding_dim, hidden_dim_1)\n",
    "        self.hidden_2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim_2, 1)\n",
    "\n",
    "    def forward(self, titles):\n",
    "        with torch.no_grad():  #To ensure no gradients are computed for the embedding model\n",
    "            embeddings = self.embedding_model.embed(titles)\n",
    "        pooled_embeddings = embeddings.mean(dim=1)  # Now x is of shape (batch_size, embedding_dim)\n",
    "        hidden_1_embeddings = self.hidden_1(pooled_embeddings)\n",
    "        activated_1_embeddings = self.relu(hidden_1_embeddings)\n",
    "        hidden_2_embeddings = self.hidden_2(activated_1_embeddings)\n",
    "        activated_2_embeddings = self.relu(hidden_2_embeddings)\n",
    "        score_predictions = self.output(activated_2_embeddings)\n",
    "        return score_predictions\n",
    "    \n",
    "    def predict(self, title):\n",
    "        self.eval()\n",
    "        self.forward(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim_1= 64\n",
    "hidden_dim_2 = 32\n",
    "sp_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CBOW version of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "sp_cbow_model = ScorePredictor(vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, model = CBOWModel, weights = \"cbow_weights/cbow_epoch_5.pth\")\n",
    "sp_cbow_loss_function = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "sp_cbow_optimizer = optim.Adam(sp_cbow_model.parameters(), lr=learning_rate)  # Using Adam optimizer\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(sp_epochs):\n",
    "    total_loss = 0\n",
    "    # Wrap your dataloader with tqdm for a progress bar\n",
    "    for inputs, targets in tqdm(sp_dataloader, desc=f'Epoch {epoch+1}/{sp_epochs}'):\n",
    "        \n",
    "        inputs = inputs.long()  \n",
    "        sp_cbow_model.zero_grad()\n",
    "        outputs = sp_cbow_model(inputs)\n",
    "        loss = sp_cbow_loss_function(outputs, targets.view(outputs.size()))  # Compute loss, ensuring target shape matches output\n",
    "        loss.backward()  # Backpropagation\n",
    "        sp_cbow_optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch {epoch+1}/{sp_epochs}, Loss: {total_loss/len(sp_dataloader)}')\n",
    "    \n",
    "    # Save the model state after each epoch\n",
    "    torch.save(sp_cbow_model.state_dict(), os.path.join(\"sp_cbow_weights\", f\"sp_cbow_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Skipgram version of Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "sp_skipgram_model = ScorePredictor(vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, model = SkipGramModel, weights = \"skipgram_weights/skipgram_epoch_5.pth\")\n",
    "sp_skipgram_loss_function = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "sp_skipgram_optimizer = optim.Adam(sp_skipgram_model.parameters(), lr=learning_rate)  # Using Adam optimizer\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(sp_epochs):\n",
    "    total_loss = 0\n",
    "    # Wrap your dataloader with tqdm for a progress bar\n",
    "    for inputs, targets in tqdm(sp_dataloader, desc=f'Epoch {epoch+1}/{sp_epochs}'):\n",
    "        \n",
    "        inputs = inputs.long()  \n",
    "        sp_skipgram_model.zero_grad()\n",
    "        outputs = sp_skipgram_model(inputs)\n",
    "        loss = sp_skipgram_loss_function(outputs, targets.view(outputs.size()))  # Compute loss, ensuring target shape matches output\n",
    "        loss.backward()  # Backpropagation\n",
    "        sp_skipgram_optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch {epoch+1}/{sp_epochs}, Loss: {total_loss/len(sp_dataloader)}')\n",
    "    \n",
    "    # Save the model state after each epoch\n",
    "    torch.save(sp_skipgram_model.state_dict(), os.path.join(\"sp_skipgram_weights\", f\"sp_skipgram_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for using our model to perform inference\n",
    "def score_predictor(title, vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, embed_model, embed_weights, weights):\n",
    "    model = ScorePredictor(vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, embed_model, embed_weights)\n",
    "    model.load_state_dict(torch.load(weights))\n",
    "    \n",
    "    title_ids = (torch.tensor([torch.tensor(sp.piece_to_id(token)) for token in sp.encode_as_pieces(title)])).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():  #To ensure no gradients are computed for the embedding model\n",
    "        score = model.predict(title_ids)\n",
    "\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing inference on the fake example title \"Hello world\"\n",
    "score_prediction = score_predictor(\"Hello world\", vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, embed_model=SkipGramModel, embed_weights=\"skipgram_weights/skipgram_epoch_5.pth\", weights=\"sp_skipgram_weights/sp_skipgram_epoch_10.pth\")\n",
    "print(score_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLX4.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
